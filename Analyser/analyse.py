from Utils import add_logger_to_class
@add_logger_to_class
class Analyse():
    def __init__(self,score_dict) -> None:
        self.score_dict=score_dict
    
    def analyse(self):
        """
        A function to analyse the score that LLM gets in the testproject, including many testcases.
        The information used for analysis comes from the function get_eval_result in class getinfo.
        By reading the information(a dictionary) provided by the function get_eval_result, this function will create a new log file and write the analysis in it.
        The avarage socre that the LLM gets in testcase will be recorded, and finally the function will give an overall evaluation of the LLM.
        The log file generated by this function is formatted like: 
        "By 'keywords' evaluation, the LLM gets XX(0-1) scores in average.
         By 'toolUsage' evaluation, the LLM gets XX(0-1) scores in average.
         By 'gpt4Eval' evaluation, the LLM gets XX(0-1) scores in average.
         To conclude, the LLM â€¦"
        
        """
        score=self.score_dict
        score_list=[]
        score_mean=[]
        score_list.append(score['keywords'])
        score_list.append(score['toolUsage'])
        score_list.append(score['gpt4Eval'])
        score_list.append(score['blacklist'])
        for score in score_list:
            if score==[]:
                score_mean.append('Not evaluated by this method')
            else:
                score_mean.append(sum(score)/len(score))

        mean_score_info=f'By \'keywords\' evaluation, the LLM gets {score_mean[0]} scores in average.\nBy \'toolUsage\' evaluation, the LLM gets {score_mean[1]} scores in average.\nBy \'gpt4Eval\' evaluation, the LLM gets {score_mean[2]} scores in average.\nBy \'blacklist\' evaluation, the LLM gets {score_mean[3]} scores in average.\n'
        print(mean_score_info)
        mean_score_list=[score_mean[0],score_mean[1],score_mean[2],score_mean[3]]
        for score in mean_score_list:
            if score>=0.6:
                score='does well in'
            else:
                score='is not good at'
        sum_info=f'To conclude: The model {mean_score_list[0]} in \'keywords\' evaluation.\nThe model {mean_score_list[1]} in \'toolUsage\' evaluation.\nThe model {mean_score_list[2]} in \'gpt4Eval\' evaluation.\nThe model {mean_score_list[3]} in \'blacklist\' evaluation.\n'
        print(sum_info)








        
